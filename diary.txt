Term 1

Week 1: I spent week 1 trying to understand how to use Hadoop and familiarising myself with its syntax. I borrowed a book from the library (Hadoop: the definitive guide) and watched a couple of YouTube videos in order to gain an understanding of the basics of Hadoop. Thereafter, to practise my knowledge, I tried unsuccessfully to setup Hadoop on my local system for which I suspect that my environment variables were not setup the correct way. Therefore, I started using noMachine in order to familiarise myself with its usage since it easily allows you to connect to the big data system at RHUL. Moreover, I noticed that my git repository has not been created so I emailed the CIM helpdesk team to sort out the problem.

Week 2: I spent week 2 working on problem 1. During this week, all my efforts were dedicated to writing a java file such that it takes a text file as an input and outputs the number of occurrences for each word contained in that text file. Although I did not specify that a text file was going to be the input, I still continued with the idea since I received approval from my supervisor. The task itself was not difficult at all as I have tackled similiar tasks in the past. On a side note, I used hashmaps over the traditional Mapper/Reducer classes offered by Hadoop, hence, I had to start again and implement what I implemented with Hashmaps, onto a new class with a Mapper and Reducer.

Week 3: During the start of week 3, I modified the code for problem 1 such that it now takes a few HTML pages (poetry related webpages) as input and then outputs the occurence of each word in the poem. The task proved to be a bit more difficult than previously thought since I had to perform the counting of each word for only a section of the webpage (the poem itself). To tackle this problem, I used regex so that the algorithm is only performed when it encounters the div with id="post_description" and it stops after its closure. Moreover, I used regex in order to omit any words starting with "href" and any HTML tags from the word count.

Week 4: I spent the entirety of this week working on problem 2 and I managed to implement a basic distributed grep functionality that extracts each word that matches a specific regex expression ("in this case any word that contains h") and output the word alongside its line number. However, the line number is not accurate and there may be a logical error in the code. However, the basic functionality for the code is there.

Week 5: During this week, I tried to fix the error relating to the line number, albeit I was not successful. However, I made slight modifications to the code such that it now extract each word matching the regex in question from a section of the HTML page. Indeed, the section of the HTML page is enclosed by "id="post_description" and a "div", which allows me to extract only the necessary information. Furthermore, I made slight changes to the WordCount.java file.

Week 6: I spent this week trying to figure out how to output he correct line number but after multiple attempts it outputs a slightly different number from what expected. I opted to use a while loop and iterate through each string that matched the regex expression in question, afterwards a variable named linenumber was incremented. However, this method does not seem to work. Moreover, I uploaded 5 more HTML pages containing more text to the repository, they are going to serve for testing.

Week 7: During a FYP meeting, my supervisor suggested the idea of using two mappers and chaining them together in order to output the current line number, however, after many efforts, I was unsuccessful. I researched this topic a lot and I tried to chain two mappers but it proved very complicated to implement for a program that implements a distributed grep functionality. I was unsuccessful at finding any resources online that could help me with this problem.

Week 8: During this week, I managed to finish problem 2 completely and I started working on my interim report. Indeed, during a meeting with my supervisor, it turned out that there was a misunderstanding and the idea of implementing 2 mappers and chaining them together was ineffective for task 2. Moreover, it turns out that the key input for FIleInputFormat for a mapper represented the byteOffset of the line in question. Indeed, through some research online, I managed to implement "LineNumberInputFormat" which extends FileInputFormat such that instead of displaying the byteOffset, it now displays the actual line number. In order to implement this functionality, I borrowed sections of code online as this functionality was not inherently requsted by my supervisor as well as for the fact that I did not have any knowledge regarding the class FileINputFormat.  

Week 9: During this week, I started to exclusively dedicate all my efforts towards finishing off most of the sections in my report. In fact, I managed to finish certain sections such as the Abstract and the entirety of Project Progression. Also, I started working on my presentation and I successfully managed to submit it before the deadline. Moreover, I submitted a couple of drafts to my supervisor and I received valuable feedback that helped me improve my report.

Week 10: During this week, most of my efforts were concentrated towards finishing up my report and preparing myself for my presentation. Indeed, I spent the first few days memorising a script to recite during my presentation, which ended up going Ok. I finished the interim report during this week.

Term 2

Week 1: During this week I started experimenting with the dataset "NuclearDecay.csv" and made some progress regarding the calculation of the mean for the variable "massExcessUncertainty". However, not much progress has been made since I was unsure how to calculate other variables from the same java file.

Week 2: During this week, I partially managed to calculate the mode, that is, I calculated the cumulative frequencies for each of the values in the "massExcessUncertainty" column and outputted them using mapReduce. However, calculating the mode proved to be a bit difficult as many calculation methods proved to be unsuccessful.

Week 3: During this week, I looked up resources on how to calculate the mode whilst utilising only the mapper and reducer classes but this proved to be quite difficult to implement in hadoop for my specific scenario. Due to this reason, I had trouble making any progress for this week.

Week 4: During this week, I looked up resources for ways in order to output mutilple calculations under one single output file and after some research and some talks with a few other students, I found out that it was possible to context.write() many times and that there existed a method called cleanup which is called after all the map tasks a re completed.

Week 5: During this week, I managed to calculate the minimum and maximum values for the variable "massExcessUncertainty" from the dataset "NuclearDecay.csv". I decided that I was going to calculate all variables separetely in different java files and congregate everything at the end.

Week 6: During this week, I managed to calculate the median and mode for the NuclearDecay.csv dataset separetely (in different java files). However, at the end of this week I tried to aggregrate all the separate variable calculations under one single java file, which was successful to a certain extent.

Week 7: During this week, I managed to finally implement the mode and median calculations into NuclearDecay.java. Thus, I managed to get the java file to output all variables under a single file. In the end, I deleted any unnecessary java files.

Week 8: During this week, I wrote a java file that calculated the cumulative frequencies for each of the values in the "massExcessUncertainty" column and I used python to generate a graph based on those values. Its important to note that I reused the code that I wrote for the parital calculation of the mode for problem 3 and I borrowed a code snippets regarding the calculation of the interval and length of the bars as I was not too familiar with matplotlib (only using it a few times for my machine learning module)

Week 9: During this week, I managed to calculate the daily price range and change for a certain stock from a NASDAQ dataset. This calculation did not prove to be difficult at all since it was very basic. Moreover, I generated a graph for the calculation of the price range/change using python.

Week 10: During this week, I managed to calculate the volume rate of change for the same stock and generated a graph using python in order to visualise the daily volume rate of change for that stock. The calculations were not too complicated as the equation for this calculation was basic. Moreover, during this week I started to work on my report.

Week 11: By this week, most of the code was finished as I managed to calculate the final metric for problem 4 which was the RSI. This calculation proved to be a bit more complicated than the others as the equation can be confusing, nevertheless, after some research it did not prove to be too difficult. Moreover, By this week I was nearly finished with the Rationale, the Professional Issues and the Literature Review sections of my code. However, my final draft to my supervisor was somewhat incomplete and the feedback I received was limited.

Week 12: During this week, I mainly worked on the remaining sections of my report. However, I also made slight changes to my shell script and to my RegexSearch.java file such that it is now capable of taking a regex expression as input directly from the command line.